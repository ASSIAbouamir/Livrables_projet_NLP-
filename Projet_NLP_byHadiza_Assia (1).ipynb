{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMzYB6YysURY",
        "outputId": "e1757a5e-f017-4fc4-9367-a211398809f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.2.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.4.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq7Fr2ciroYw",
        "outputId": "12ff5cbc-16f5-483b-e2c4-83b843858cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_001.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_002.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_003.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_004.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_005.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_006.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_007.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_008.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_009.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_010.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_011.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_012.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_013.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_014.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_015.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_016.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_017.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_018.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_019.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_020.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_021.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_022.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_023.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_024.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_025.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_026.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_027.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_028.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_029.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_030.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_031.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_032.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_033.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_034.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_035.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_036.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_037.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_038.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_039.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_040.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_041.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_042.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_043.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_044.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_045.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_046.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_047.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_048.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_049.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_050.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_051.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_052.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_053.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_054.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_055.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_056.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_057.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_058.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_059.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_060.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_061.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_062.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_063.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_064.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_065.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_066.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_067.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_068.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_069.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_070.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_071.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_072.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_073.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_074.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_075.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_076.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_077.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_078.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_079.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_080.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_081.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_082.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_083.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_084.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_085.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_086.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_087.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_088.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_089.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_090.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_091.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_092.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_093.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_094.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_095.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_096.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_097.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_098.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_099.pdf\n",
            "Généré → /content/drive/MyDrive/cv_pdfs/cv_100.pdf\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2) Imports & configuration\n",
        "import os\n",
        "import random\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "# Nombre de CVs à générer\n",
        "N = 100  # ajustez selon vos besoins\n",
        "\n",
        "# Dossier de sortie dans votre Drive\n",
        "output_dir = '/content/drive/MyDrive/cv_pdfs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 3) Jeux de données réalistes\n",
        "noms       = [\"Dupont\", \"Moreau\", \"Leroy\", \"Fontaine\", \"Blanc\"]\n",
        "prenoms    = [\"Jean\", \"Marie\", \"Luc\", \"Claire\", \"Élise\"]\n",
        "emails     = [\"{p}.{n}@example.com\".format(p=p.lower(), n=n.lower()) for p, n in zip(prenoms, noms)]\n",
        "telephones = [\"06 {:02d} {:02d} {:02d} {:02d}\".format(*random.sample(range(100), 4)) for _ in noms]\n",
        "\n",
        "profils = [\n",
        "    \"Professionnel motivé avec une forte expérience dans le domaine de la finance.\",\n",
        "    \"Ingénieure informatique spécialisée en cybersécurité et analyse de données.\",\n",
        "    \"Chef de projet expérimenté dans la gestion de projets internationaux.\",\n",
        "    \"Développeur web passionné par les technologies front-end.\",\n",
        "    \"Assistante RH avec 5 ans d'expérience en recrutement et formation.\"\n",
        "]\n",
        "\n",
        "experiences = [\n",
        "    [\"Analyste financier chez BNP Paribas (2018-2022)\",\n",
        "     \"Consultant junior chez KPMG (2016-2018)\"],\n",
        "    [\"Ingénieur réseau chez Orange (2020-2023)\",\n",
        "     \"Technicien réseau chez SFR (2017-2020)\"],\n",
        "    [\"Chef de projet chez Capgemini (2017-2021)\",\n",
        "     \"Assistant chef de projet chez Accenture (2015-2017)\"],\n",
        "    [\"Développeur web chez Sopra Steria (2019-2022)\",\n",
        "     \"Stagiaire dev chez Ubisoft (2018)\"],\n",
        "    [\"Chargée de recrutement chez Adecco (2016-2020)\",\n",
        "     \"Assistante RH chez Randstad (2014-2016)\"]\n",
        "]\n",
        "\n",
        "formations = [\n",
        "    [\"Master Finance, Université Paris-Dauphine (2016)\",\n",
        "     \"Licence Économie, Université de Lille (2014)\"],\n",
        "    [\"Diplôme d'ingénieur, INSA Lyon (2019)\",\n",
        "     \"Classes prépa, Lycée Louis-le-Grand (2016)\"],\n",
        "    [\"Master Management, ESCP (2015)\",\n",
        "     \"Licence Gestion, Université de Nantes (2013)\"],\n",
        "    [\"Licence Informatique, Sorbonne Université (2018)\",\n",
        "     \"BTS SN (Systèmes Numériques), Lycée Turgot (2016)\"],\n",
        "    [\"BTS RH, Lycée Turgot (2014)\",\n",
        "     \"Licence Psychologie, Université Rennes 2 (2012)\"]\n",
        "]\n",
        "\n",
        "competences = [\n",
        "    [\"Python\", \"SQL\", \"Excel\"],\n",
        "    [\"Sécurité réseau\", \"Linux\", \"Cisco\"],\n",
        "    [\"Gestion de projet\", \"Agile\", \"Scrum\"],\n",
        "    [\"HTML\", \"CSS\", \"JavaScript\"],\n",
        "    [\"Recrutement\", \"Formation\", \"Paie\"]\n",
        "]\n",
        "\n",
        "langues = [\n",
        "    [\"Français (natif)\", \"Anglais (courant)\"],\n",
        "    [\"Français\", \"Espagnol (B2)\"],\n",
        "    [\"Français\", \"Anglais (professionnel)\", \"Allemand (B1)\"],\n",
        "    [\"Français\", \"Italien (intermédiaire)\"],\n",
        "    [\"Anglais\", \"Arabe (bilingue)\"]\n",
        "]\n",
        "\n",
        "certificats = [\n",
        "    [\"PMP\", \"TOEIC 950\"],\n",
        "    [\"CCNA\", \"Google Data Analyst\"],\n",
        "    [\"Scrum Master\", \"Certificat Voltaire\"],\n",
        "    [\"HubSpot Inbound\", \"Michelin Certification\"],\n",
        "    [\"AWS Certified\", \"Cisco CCNP\"]\n",
        "]\n",
        "\n",
        "projets = [\n",
        "    [\"Application mobile de gestion budgétaire\",\n",
        "     \"Plateforme e-learning open source\"],\n",
        "    [\"Refonte UI d’une application mobile\",\n",
        "     \"Chatbot RH en Python\"],\n",
        "    [\"Automatisation de reporting Excel\",\n",
        "     \"Dashboard Power BI\"],\n",
        "    [\"Site web vitrine pour ONGs\",\n",
        "     \"Migration CRM cloud\"],\n",
        "    [\"Plateforme de e-commerce\",\n",
        "     \"Système de tagging sémantique\"]\n",
        "]\n",
        "\n",
        "interets = [\n",
        "    [\"Lecture\", \"Voyage\", \"Photographie\"],\n",
        "    [\"Yoga\", \"Gastronomie\", \"Cinéma\"],\n",
        "    [\"Musique\", \"Coding\", \"Randonnée\"],\n",
        "    [\"Théâtre\", \"Peinture\", \"Cyclisme\"],\n",
        "    [\"Jeux vidéo\", \"Bénévolat\", \"Dessins\"]\n",
        "]\n",
        "\n",
        "references = [\n",
        "    [\"M. Martin – DAF BNP Paribas\", \"Mme Lefevre – DRH Orange\"],\n",
        "    [\"M. Petit – CTO Capgemini\", \"Mme Durand – Manager KPMG\"],\n",
        "    [\"M. Moreau – Lead Dev Ubisoft\", \"M. Garnier – BI Société Générale\"],\n",
        "    [\"Mme Laurent – RH L'Oréal\", \"M. Dubois – Consultant SFR\"],\n",
        "    [\"Mme Colin – UX Ubisoft\", \"M. Roussel – Directeur RH Adecco\"]\n",
        "]\n",
        "\n",
        "# 4) Fonction de génération d’un CV PDF\n",
        "def generate_cv_pdf(cv, filepath):\n",
        "    c = canvas.Canvas(filepath, pagesize=A4)\n",
        "    w, h = A4\n",
        "    y = h - 50\n",
        "\n",
        "    # Entête\n",
        "    c.setFont(\"Helvetica-Bold\", 14)\n",
        "    c.drawString(50, y, f\"{cv['prenom']} {cv['nom']}\")\n",
        "    y -= 20\n",
        "    c.setFont(\"Helvetica\", 11)\n",
        "    c.drawString(50, y, f\"Email      : {cv['email']}\")\n",
        "    y -= 15\n",
        "    c.drawString(50, y, f\"Téléphone: {cv['telephone']}\")\n",
        "    y -= 25\n",
        "\n",
        "    # Sections\n",
        "    def draw(title, content):\n",
        "        nonlocal y\n",
        "        c.setFont(\"Helvetica-Bold\", 12)\n",
        "        c.drawString(50, y, title)\n",
        "        y -= 15\n",
        "        c.setFont(\"Helvetica\", 11)\n",
        "        if isinstance(content, list):\n",
        "            for line in content:\n",
        "                if y < 50:\n",
        "                    c.showPage(); y = h - 50\n",
        "                c.drawString(60, y, f\"- {line}\")\n",
        "                y -= 12\n",
        "        else:\n",
        "            if y < 50:\n",
        "                c.showPage(); y = h - 50\n",
        "            c.drawString(60, y, content)\n",
        "            y -= 12\n",
        "        y -= 8\n",
        "\n",
        "    draw(\"Profil\",       cv[\"profil\"])\n",
        "    draw(\"Expérience\",   cv[\"experience\"])\n",
        "    draw(\"Formation\",    cv[\"formation\"])\n",
        "    draw(\"Compétences\",  cv[\"competences\"])\n",
        "    draw(\"Langues\",      cv[\"langues\"])\n",
        "    draw(\"Certificats\",  cv[\"certificats\"])\n",
        "    draw(\"Projets\",      cv[\"projets\"])\n",
        "    draw(\"Intérêts\",     cv[\"interets\"])\n",
        "    draw(\"Références\",   cv[\"references\"])\n",
        "\n",
        "    c.save()\n",
        "\n",
        "# 5) Génération de N CVs\n",
        "for i in range(1, N+1):\n",
        "    idx = random.randrange(len(noms))\n",
        "    cv_data = {\n",
        "        \"nom\":         noms[idx],\n",
        "        \"prenom\":      prenoms[idx],\n",
        "        \"email\":       emails[idx],\n",
        "        \"telephone\":   telephones[idx],\n",
        "        \"profil\":      profils[idx],\n",
        "        \"experience\":  experiences[idx],\n",
        "        \"formation\":   formations[idx],\n",
        "        \"competences\": competences[idx],\n",
        "        \"langues\":     langues[idx],\n",
        "        \"certificats\": certificats[idx],\n",
        "        \"projets\":     projets[idx],\n",
        "        \"interets\":    interets[idx],\n",
        "        \"references\":  references[idx]\n",
        "    }\n",
        "    out_path = os.path.join(output_dir, f\"cv_{i:03d}.pdf\")\n",
        "    generate_cv_pdf(cv_data, out_path)\n",
        "    print(f\"Généré → {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF spacy\n",
        "!python -m spacy download fr_core_news_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqoI9qSmt_Qe",
        "outputId": "d2308f72-1667-49d5-e1ef-8d9f82b944ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n",
            "Collecting fr-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz                               # pip install PyMuPDF\n",
        "import spacy                             # pip install spacy && python -m spacy download fr_core_news_sm\n",
        "from collections import OrderedDict\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "PDF_FOLDER  = \"/content/drive/MyDrive/cv_pdfs\"    # dossier contenant les CVs PDF\n",
        "OUTPUT_JSON = \"labelstudio_preannotated.json\"\n",
        "SPACY_MODEL = \"fr_core_news_md\"\n",
        "\n",
        "# --- Chargement spaCy français ---\n",
        "nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "# --- Regex pour sections CV ---\n",
        "SECTION_PATTERNS = OrderedDict([\n",
        "    (\"PROFIL\",       [r'(?i)\\b(PROFIL|RÉSUMÉ|PRÉSENTATION)\\b']),\n",
        "    (\"EXPERIENCE\",   [r'(?i)\\b(EXPÉRIENCE|PARCOURS)\\b']),\n",
        "    (\"FORMATION\",    [r'(?i)\\b(FORMATION|DIPLÔMES|ÉTUDES)\\b']),\n",
        "    (\"COMPETENCES\",  [r'(?i)\\b(COMPÉTENCES|SAVOIR[- ]FAIRE)\\b']),\n",
        "    (\"LANGUES\",      [r'(?i)\\b(LANGUES)\\b']),\n",
        "    (\"CERTIFICATS\",  [r'(?i)\\b(CERTIFICATS?|CERTIFICATIONS?)\\b']),\n",
        "    (\"PROJETS\",      [r'(?i)\\b(PROJETS|RÉALISATIONS)\\b']),\n",
        "    (\"INTERETS\",     [r'(?i)\\b(INTÉRÊTS|LOISIRS)\\b']),\n",
        "    (\"REFERENCES\",   [r'(?i)\\b(RÉFÉRENCES|RECOMMANDATIONS)\\b']),\n",
        "    (\"COORDONNEES\",  [r'(?i)\\b(COORDONNÉES|CONTACT)\\b'])\n",
        "])\n",
        "\n",
        "# --- Regex pour email & téléphone ---\n",
        "EMAIL_RE = re.compile(r\"[a-zA-Z0-9.\\-_]+@[a-zA-Z0-9.\\-_]+\\.[a-zA-Z]{2,}\")\n",
        "TEL_RE   = re.compile(r\"\\+?\\d[\\d\\-\\s]{7,}\\d\")\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    \"\"\"Renvoie le texte brut d’un PDF entier.\"\"\"\n",
        "    txt = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            txt += page.get_text()\n",
        "    return txt\n",
        "\n",
        "def find_sections(text):\n",
        "    \"\"\"\n",
        "    Retourne un OrderedDict {section_name: (start, end)} en estimant\n",
        "    chaque section entre son titre et le titre suivant.\n",
        "    \"\"\"\n",
        "    matches = []\n",
        "    for name, patterns in SECTION_PATTERNS.items():\n",
        "        for pat in patterns:\n",
        "            for m in re.finditer(pat, text):\n",
        "                matches.append((m.start(), name))\n",
        "    # tri par position\n",
        "    matches.sort(key=lambda x: x[0])\n",
        "    sections = OrderedDict()\n",
        "    for idx, (pos, name) in enumerate(matches):\n",
        "        start = pos\n",
        "        end = matches[idx+1][0] if idx+1 < len(matches) else len(text)\n",
        "        sections.setdefault(name, []).append((start, end))\n",
        "    return sections\n",
        "\n",
        "def annotate(text):\n",
        "    \"\"\"\n",
        "    Retourne la liste d'annotations auto :\n",
        "    - Section spans (labels de type 'section')\n",
        "    - Entités (NOM, PRÉNOM, EMAIL, TÉLÉPHONE)\n",
        "    \"\"\"\n",
        "    annots = []\n",
        "\n",
        "    # 1) Sections\n",
        "    sections = find_sections(text)\n",
        "    for sec_name, spans in sections.items():\n",
        "        for start, end in spans:\n",
        "            annots.append({\n",
        "                \"start\": start,\n",
        "                \"end\":   end,\n",
        "                \"text\":  text[start:end].strip(),\n",
        "                \"label\": sec_name\n",
        "            })\n",
        "\n",
        "    # 2) Personnes via spaCy\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PER\":\n",
        "            parts = ent.text.split()\n",
        "            if len(parts) >= 2:\n",
        "                # prénom\n",
        "                annots.append({\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\":   ent.start_char + len(parts[0]),\n",
        "                    \"text\":  parts[0],\n",
        "                    \"label\": \"PRÉNOM\"\n",
        "                })\n",
        "                # nom\n",
        "                annots.append({\n",
        "                    \"start\": ent.end_char - len(parts[-1]),\n",
        "                    \"end\":   ent.end_char,\n",
        "                    \"text\":  parts[-1],\n",
        "                    \"label\": \"NOM\"\n",
        "                })\n",
        "            else:\n",
        "                annots.append({\n",
        "                    \"start\": ent.start_char,\n",
        "                    \"end\":   ent.end_char,\n",
        "                    \"text\":  ent.text,\n",
        "                    \"label\": \"NOM_PRÉNOM\"\n",
        "                })\n",
        "\n",
        "    # 3) Email\n",
        "    for m in EMAIL_RE.finditer(text):\n",
        "        annots.append({\"start\": m.start(), \"end\": m.end(), \"text\": m.group(), \"label\": \"EMAIL\"})\n",
        "\n",
        "    # 4) Téléphone\n",
        "    for m in TEL_RE.finditer(text):\n",
        "        annots.append({\"start\": m.start(), \"end\": m.end(), \"text\": m.group(), \"label\": \"TÉLÉPHONE\"})\n",
        "\n",
        "    # On peut ici ajouter d'autres regex (URL, entreprise, date, etc.)\n",
        "\n",
        "    # Déduplication par (start,end,label)\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for a in sorted(annots, key=lambda x: (x[\"start\"], x[\"end\"], x[\"label\"])):\n",
        "        key = (a[\"start\"], a[\"end\"], a[\"label\"])\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique.append(a)\n",
        "    return unique\n",
        "\n",
        "# --- Construction des tâches Label Studio ---\n",
        "tasks = []\n",
        "for fname in os.listdir(PDF_FOLDER):\n",
        "    if not fname.lower().endswith(\".pdf\"):\n",
        "        continue\n",
        "    path = os.path.join(PDF_FOLDER, fname)\n",
        "    txt  = extract_text(path)\n",
        "    if not txt.strip():\n",
        "        continue\n",
        "\n",
        "    auto_ann = annotate(txt)\n",
        "    results = []\n",
        "    for a in auto_ann:\n",
        "        results.append({\n",
        "            \"from_name\": \"label\",\n",
        "            \"to_name\":   \"text\",\n",
        "            \"type\":      \"labels\",\n",
        "            \"value\": {\n",
        "                \"start\":  a[\"start\"],\n",
        "                \"end\":    a[\"end\"],\n",
        "                \"text\":   a[\"text\"],\n",
        "                \"labels\": [a[\"label\"]]\n",
        "            }\n",
        "        })\n",
        "\n",
        "    tasks.append({\n",
        "        \"data\":        {\"text\": txt},\n",
        "        \"annotations\":[{\"result\": results}]\n",
        "    })\n",
        "    print(f\"Pré-annoté {fname} → {len(results)} entités\")\n",
        "\n",
        "# --- Sauvegarde en JSON compatible Label Studio ---\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tasks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Prêt à importer dans Label Studio : {OUTPUT_JSON}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f41r8RG1uIbC",
        "outputId": "015e3473-b026-40b2-c416-cd61db2930bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pré-annoté cv_001.pdf → 17 entités\n",
            "Pré-annoté cv_002.pdf → 17 entités\n",
            "Pré-annoté cv_003.pdf → 17 entités\n",
            "Pré-annoté cv_004.pdf → 17 entités\n",
            "Pré-annoté cv_005.pdf → 17 entités\n",
            "Pré-annoté cv_006.pdf → 17 entités\n",
            "Pré-annoté cv_007.pdf → 24 entités\n",
            "Pré-annoté cv_008.pdf → 17 entités\n",
            "Pré-annoté cv_009.pdf → 17 entités\n",
            "Pré-annoté cv_010.pdf → 17 entités\n",
            "Pré-annoté cv_011.pdf → 24 entités\n",
            "Pré-annoté cv_012.pdf → 24 entités\n",
            "Pré-annoté cv_013.pdf → 18 entités\n",
            "Pré-annoté cv_014.pdf → 24 entités\n",
            "Pré-annoté cv_015.pdf → 17 entités\n",
            "Pré-annoté cv_016.pdf → 24 entités\n",
            "Pré-annoté cv_017.pdf → 24 entités\n",
            "Pré-annoté cv_018.pdf → 24 entités\n",
            "Pré-annoté cv_019.pdf → 24 entités\n",
            "Pré-annoté cv_020.pdf → 24 entités\n",
            "Pré-annoté cv_021.pdf → 18 entités\n",
            "Pré-annoté cv_022.pdf → 24 entités\n",
            "Pré-annoté cv_023.pdf → 17 entités\n",
            "Pré-annoté cv_024.pdf → 17 entités\n",
            "Pré-annoté cv_025.pdf → 17 entités\n",
            "Pré-annoté cv_026.pdf → 17 entités\n",
            "Pré-annoté cv_027.pdf → 17 entités\n",
            "Pré-annoté cv_028.pdf → 17 entités\n",
            "Pré-annoté cv_029.pdf → 17 entités\n",
            "Pré-annoté cv_030.pdf → 24 entités\n",
            "Pré-annoté cv_031.pdf → 17 entités\n",
            "Pré-annoté cv_032.pdf → 17 entités\n",
            "Pré-annoté cv_033.pdf → 17 entités\n",
            "Pré-annoté cv_034.pdf → 24 entités\n",
            "Pré-annoté cv_035.pdf → 17 entités\n",
            "Pré-annoté cv_036.pdf → 18 entités\n",
            "Pré-annoté cv_037.pdf → 24 entités\n",
            "Pré-annoté cv_038.pdf → 17 entités\n",
            "Pré-annoté cv_039.pdf → 17 entités\n",
            "Pré-annoté cv_040.pdf → 17 entités\n",
            "Pré-annoté cv_041.pdf → 24 entités\n",
            "Pré-annoté cv_042.pdf → 18 entités\n",
            "Pré-annoté cv_043.pdf → 17 entités\n",
            "Pré-annoté cv_044.pdf → 17 entités\n",
            "Pré-annoté cv_045.pdf → 18 entités\n",
            "Pré-annoté cv_046.pdf → 17 entités\n",
            "Pré-annoté cv_047.pdf → 17 entités\n",
            "Pré-annoté cv_048.pdf → 17 entités\n",
            "Pré-annoté cv_049.pdf → 24 entités\n",
            "Pré-annoté cv_050.pdf → 18 entités\n",
            "Pré-annoté cv_051.pdf → 17 entités\n",
            "Pré-annoté cv_052.pdf → 24 entités\n",
            "Pré-annoté cv_053.pdf → 17 entités\n",
            "Pré-annoté cv_054.pdf → 17 entités\n",
            "Pré-annoté cv_055.pdf → 24 entités\n",
            "Pré-annoté cv_056.pdf → 17 entités\n",
            "Pré-annoté cv_057.pdf → 24 entités\n",
            "Pré-annoté cv_058.pdf → 18 entités\n",
            "Pré-annoté cv_059.pdf → 18 entités\n",
            "Pré-annoté cv_060.pdf → 17 entités\n",
            "Pré-annoté cv_061.pdf → 24 entités\n",
            "Pré-annoté cv_062.pdf → 17 entités\n",
            "Pré-annoté cv_063.pdf → 17 entités\n",
            "Pré-annoté cv_064.pdf → 24 entités\n",
            "Pré-annoté cv_065.pdf → 17 entités\n",
            "Pré-annoté cv_066.pdf → 17 entités\n",
            "Pré-annoté cv_067.pdf → 18 entités\n",
            "Pré-annoté cv_068.pdf → 18 entités\n",
            "Pré-annoté cv_069.pdf → 18 entités\n",
            "Pré-annoté cv_070.pdf → 17 entités\n",
            "Pré-annoté cv_071.pdf → 17 entités\n",
            "Pré-annoté cv_072.pdf → 17 entités\n",
            "Pré-annoté cv_073.pdf → 18 entités\n",
            "Pré-annoté cv_074.pdf → 17 entités\n",
            "Pré-annoté cv_075.pdf → 17 entités\n",
            "Pré-annoté cv_076.pdf → 17 entités\n",
            "Pré-annoté cv_077.pdf → 18 entités\n",
            "Pré-annoté cv_078.pdf → 17 entités\n",
            "Pré-annoté cv_079.pdf → 17 entités\n",
            "Pré-annoté cv_080.pdf → 17 entités\n",
            "Pré-annoté cv_081.pdf → 18 entités\n",
            "Pré-annoté cv_082.pdf → 17 entités\n",
            "Pré-annoté cv_083.pdf → 24 entités\n",
            "Pré-annoté cv_084.pdf → 24 entités\n",
            "Pré-annoté cv_085.pdf → 17 entités\n",
            "Pré-annoté cv_086.pdf → 24 entités\n",
            "Pré-annoté cv_087.pdf → 17 entités\n",
            "Pré-annoté cv_088.pdf → 17 entités\n",
            "Pré-annoté cv_089.pdf → 18 entités\n",
            "Pré-annoté cv_090.pdf → 17 entités\n",
            "Pré-annoté cv_091.pdf → 24 entités\n",
            "Pré-annoté cv_092.pdf → 17 entités\n",
            "Pré-annoté cv_093.pdf → 17 entités\n",
            "Pré-annoté cv_094.pdf → 17 entités\n",
            "Pré-annoté cv_095.pdf → 17 entités\n",
            "Pré-annoté cv_096.pdf → 18 entités\n",
            "Pré-annoté cv_097.pdf → 18 entités\n",
            "Pré-annoté cv_098.pdf → 17 entités\n",
            "Pré-annoté cv_099.pdf → 24 entités\n",
            "Pré-annoté cv_100.pdf → 17 entités\n",
            "\n",
            "✅ Prêt à importer dans Label Studio : labelstudio_preannotated.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# --- CONFIGURATION (à adapter si besoin) ---\n",
        "JSON_PATH       = \"/content/project-6-at-2025-05-16-10-30-ff5f856b.json\"\n",
        "SPACY_TRAIN     = \"train.spacy\"\n",
        "SPACY_DEV       = \"dev.spacy\"\n",
        "RANDOM_SEED     = 42\n",
        "DEV_SIZE_RATIO  = 0.1\n",
        "N_ITER          = 30\n",
        "OUTPUT_DIR      = Path(\"/content/drive/MyDrive/ModelNER_projet\")\n",
        "LANG            = \"fr\"\n",
        "\n",
        "# --- NOUVELLE CONVERSION JSON → .spacy SANS CHEVAUCHEMENTS ---\n",
        "def convert_ls_to_spacy(json_path, train_path, dev_path, dev_ratio=0.1):\n",
        "    data = json.load(open(json_path, encoding=\"utf-8\"))\n",
        "    nlp_blank = spacy.blank(LANG)\n",
        "    docs = []\n",
        "\n",
        "    for task in data:\n",
        "        text = task[\"data\"][\"text\"]\n",
        "        raw_spans = []\n",
        "        for r in task.get(\"annotations\", [])[0].get(\"result\", []):\n",
        "            s = r[\"value\"][\"start\"]\n",
        "            e = r[\"value\"][\"end\"]\n",
        "            lbl = r[\"value\"][\"labels\"][0]\n",
        "            raw_spans.append((s, e, lbl))\n",
        "\n",
        "        # Tri spans du plus court au plus long pour privilégier entités fines\n",
        "        raw_spans = sorted(raw_spans, key=lambda x: (x[1]-x[0]))\n",
        "\n",
        "        # Créer un Doc vide pour alignement\n",
        "        doc = nlp_blank.make_doc(text)\n",
        "        accepted = []\n",
        "        occupied_tokens = set()\n",
        "\n",
        "        for start, end, label in raw_spans:\n",
        "            # alignement contract pour ne jamais remonter d'erreur\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if not span:\n",
        "                continue\n",
        "            # vérif chevauchement tokène\n",
        "            tok_ids = set(range(span.start, span.end))\n",
        "            if occupied_tokens & tok_ids:\n",
        "                continue\n",
        "            accepted.append(span)\n",
        "            occupied_tokens |= tok_ids\n",
        "\n",
        "        doc.ents = accepted\n",
        "        docs.append(doc)\n",
        "\n",
        "    # split train/dev\n",
        "    random.Random(RANDOM_SEED).shuffle(docs)\n",
        "    n_dev = max(1, int(len(docs) * dev_ratio))\n",
        "    dev_docs, train_docs = docs[:n_dev], docs[n_dev:]\n",
        "\n",
        "    for docs_split, path in [(train_docs, train_path), (dev_docs, dev_path)]:\n",
        "        db = DocBin()\n",
        "        for doc in docs_split:\n",
        "            db.add(doc)\n",
        "        db.to_disk(path)\n",
        "\n",
        "\n",
        "# --- RESTE DU SCRIPT : Entraînement avec évaluation et sauvegarde du meilleur modèle ---\n",
        "def train_and_evaluate(train_path, dev_path, output_dir, n_iter=30):\n",
        "    nlp = spacy.blank(LANG)\n",
        "    ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "    # Charger les données\n",
        "    train_db = DocBin().from_disk(train_path)\n",
        "    dev_db   = DocBin().from_disk(dev_path)\n",
        "    train_docs = list(train_db.get_docs(nlp.vocab))\n",
        "    dev_docs   = list(dev_db.get_docs(nlp.vocab))\n",
        "\n",
        "    # Ajouter les labels\n",
        "    labels = set(ent.label_ for doc in train_docs for ent in doc.ents)\n",
        "    for lbl in labels:\n",
        "        ner.add_label(lbl)\n",
        "    print(\"Étiquettes NER :\", labels)\n",
        "\n",
        "    optimizer = nlp.begin_training()\n",
        "    best_f = 0.0\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for itn in range(n_iter):\n",
        "        random.shuffle(train_docs)\n",
        "        losses = {}\n",
        "        batches = minibatch(train_docs, size=compounding(4.0, 32.0, 1.5))\n",
        "        for batch in batches:\n",
        "            examples = [Example.from_dict(doc, {\n",
        "                \"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "            }) for doc in batch]\n",
        "            nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
        "\n",
        "        # Évaluation sur dev\n",
        "        scorer = Scorer()\n",
        "        for doc in dev_docs:\n",
        "            pred = nlp(doc.text)\n",
        "            # Create Example with Doc object for reference\n",
        "            example = Example(pred, doc)\n",
        "            examples.append(example)\n",
        "        scores=scorer.score(examples)\n",
        "        f_score = scores[\"ents_f\"]\n",
        "        print(f\"Epoch {itn+1}/{n_iter} — Loss: {losses.get('ner',0):.3f} — Dev F: {f_score:.3f}\")\n",
        "\n",
        "        if f_score > best_f:\n",
        "            best_f = f_score\n",
        "            nlp.to_disk(output_dir)\n",
        "            print(f\"⇧ Meilleur modèle sauvegardé (F={best_f:.3f}) → {output_dir}\")\n",
        "\n",
        "    print(f\"\\n✅ Entraînement terminé. Meilleur F-score: {best_f:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_ls_to_spacy(JSON_PATH, SPACY_TRAIN, SPACY_DEV, DEV_SIZE_RATIO)\n",
        "    train_and_evaluate(SPACY_TRAIN, SPACY_DEV, OUTPUT_DIR, N_ITER)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnJHzMdL22Dg",
        "outputId": "4deb8422-f42c-49c4-caef-ffcdcdf5dceb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Étiquettes NER : {'NOM', 'CERTIFICAT', 'PROFIL', \"CENTRE D'INTERET\", 'PRENOM', 'COMPETENCE', 'LANGUE', 'EMAIL', 'SOFT SKILLS', 'EXPERIENCE', 'TELEPHONE', 'FORMATION'}\n",
            "Epoch 1/30 — Loss: 6368.937 — Dev F: 0.843\n",
            "⇧ Meilleur modèle sauvegardé (F=0.843) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 2/30 — Loss: 5473.021 — Dev F: 0.846\n",
            "⇧ Meilleur modèle sauvegardé (F=0.846) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 3/30 — Loss: 3724.056 — Dev F: 0.847\n",
            "⇧ Meilleur modèle sauvegardé (F=0.847) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 4/30 — Loss: 2736.486 — Dev F: 0.829\n",
            "Epoch 5/30 — Loss: 2163.369 — Dev F: 0.852\n",
            "⇧ Meilleur modèle sauvegardé (F=0.852) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 6/30 — Loss: 1667.465 — Dev F: 0.858\n",
            "⇧ Meilleur modèle sauvegardé (F=0.858) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 7/30 — Loss: 1286.047 — Dev F: 0.839\n",
            "Epoch 8/30 — Loss: 1044.760 — Dev F: 0.914\n",
            "⇧ Meilleur modèle sauvegardé (F=0.914) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 9/30 — Loss: 892.883 — Dev F: 0.927\n",
            "⇧ Meilleur modèle sauvegardé (F=0.927) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 10/30 — Loss: 726.678 — Dev F: 0.919\n",
            "Epoch 11/30 — Loss: 664.433 — Dev F: 0.956\n",
            "⇧ Meilleur modèle sauvegardé (F=0.956) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 12/30 — Loss: 562.231 — Dev F: 0.943\n",
            "Epoch 13/30 — Loss: 504.209 — Dev F: 0.960\n",
            "⇧ Meilleur modèle sauvegardé (F=0.960) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 14/30 — Loss: 439.765 — Dev F: 0.959\n",
            "Epoch 15/30 — Loss: 414.907 — Dev F: 0.949\n",
            "Epoch 16/30 — Loss: 356.122 — Dev F: 0.972\n",
            "⇧ Meilleur modèle sauvegardé (F=0.972) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 17/30 — Loss: 344.756 — Dev F: 0.970\n",
            "Epoch 18/30 — Loss: 304.119 — Dev F: 0.969\n",
            "Epoch 19/30 — Loss: 297.711 — Dev F: 0.973\n",
            "⇧ Meilleur modèle sauvegardé (F=0.973) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 20/30 — Loss: 303.280 — Dev F: 0.984\n",
            "⇧ Meilleur modèle sauvegardé (F=0.984) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 21/30 — Loss: 296.250 — Dev F: 0.983\n",
            "Epoch 22/30 — Loss: 290.486 — Dev F: 0.981\n",
            "Epoch 23/30 — Loss: 266.371 — Dev F: 0.981\n",
            "Epoch 24/30 — Loss: 279.520 — Dev F: 0.975\n",
            "Epoch 25/30 — Loss: 272.528 — Dev F: 0.982\n",
            "Epoch 26/30 — Loss: 271.180 — Dev F: 0.982\n",
            "Epoch 27/30 — Loss: 279.985 — Dev F: 0.982\n",
            "Epoch 28/30 — Loss: 262.548 — Dev F: 0.984\n",
            "Epoch 29/30 — Loss: 262.430 — Dev F: 0.985\n",
            "⇧ Meilleur modèle sauvegardé (F=0.985) → /content/drive/MyDrive/ModelNER_projet\n",
            "Epoch 30/30 — Loss: 265.279 — Dev F: 0.982\n",
            "\n",
            "✅ Entraînement terminé. Meilleur F-score: 0.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy streamlit PyMuPDF pandas"
      ],
      "metadata": {
        "id": "LnLn3VkQDXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.streamlit\n",
        "\n",
        "with open(\"/root/.streamlit/config.toml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "[server]\n",
        "headless = true\n",
        "enableCORS = false\n",
        "enableXsrfProtection = false\n",
        "port = 8501\n",
        "enableWebsocketCompression = false\n",
        "browserGatherUsageStats = false\n",
        "\n",
        "[runner]\n",
        "fastReruns = false\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "K5CyNGaKmv5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tFy0a-RFrLs",
        "outputId": "f6c3f681-b9bc-4024-c3e5-aaa0c5e67742"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.106.176.205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlAkqZ64Fufc",
        "outputId": "23d12d5c-2658-499a-b914-8fe69901245b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run optimized_app_rh_cv.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fExkaDnOFxjm",
        "outputId": "ecdae110-fffb-49fd-c7ac-b19f82dcc4a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.176.205:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://giant-owls-sing.loca.lt\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2025-05-16 15:12:00.792 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-05-16 15:12:15.799 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbKWEYDEcQll",
        "outputId": "398b592b-7824-4ff2-daca-b5387d1052c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter, watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0 xlsxwriter-3.2.3\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import fitz\n",
        "import streamlit as st\n",
        "import spacy\n",
        "import csv\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Configuration de la page\n",
        "st.set_page_config(\n",
        "    page_title=\"RH Analytics - Analyse de CV\",\n",
        "    page_icon=\"📊\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Appliquer un style CSS personnalisé\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {color:#1E88E5; font-size:42px; font-weight:bold; text-align:center; margin-bottom:30px;}\n",
        "    .sub-header {color:#0D47A1; font-size:28px; font-weight:bold; margin-top:30px; margin-bottom:20px;}\n",
        "    .card {\n",
        "        background-color: #f9f9f9;\n",
        "        border-radius: 10px;\n",
        "        padding: 20px;\n",
        "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .metric-value {font-size:32px; font-weight:bold; color:#1E88E5;}\n",
        "    .metric-label {font-size:16px; color:#555;}\n",
        "    .success-box {background-color: #E8F5E9; padding:15px; border-radius:5px; border-left:5px solid #4CAF50;}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Téléchargement des ressources nécessaires\n",
        "@st.cache_resource\n",
        "def download_nltk_resources():\n",
        "    nltk.download('punkt')\n",
        "\n",
        "download_nltk_resources()\n",
        "\n",
        "# Charger les modèles SpaCy\n",
        "@st.cache_resource\n",
        "def load_spacy_models():\n",
        "    models = {}\n",
        "\n",
        "    try:\n",
        "        models['en'] = spacy.load('en_core_web_sm')\n",
        "    except:\n",
        "        st.warning(\"Modèle anglais (en_core_web_sm) non disponible. Installation en cours...\")\n",
        "        spacy.cli.download('en_core_web_sm')\n",
        "        models['en'] = spacy.load('en_core_web_sm')\n",
        "\n",
        "    try:\n",
        "        models['fr'] = spacy.load('fr_core_news_md')\n",
        "    except:\n",
        "        st.warning(\"Modèle français (fr_core_news_md) non disponible. Installation en cours...\")\n",
        "        spacy.cli.download('fr_core_news_md')\n",
        "        models['fr'] = spacy.load('fr_core_news_md')\n",
        "\n",
        "    # Tenter de charger le modèle personnalisé\n",
        "    try:\n",
        "        models['custom'] = spacy.load('/content/drive/MyDrive/ModelNER_projet')\n",
        "        st.success(\"✅ Modèle personnalisé chargé avec succès!\")\n",
        "    except:\n",
        "        models['custom'] = None\n",
        "        st.warning(\"⚠️ Modèle personnalisé non disponible. Utilisation des modèles standards.\")\n",
        "\n",
        "    # Modèles de compétences\n",
        "    try:\n",
        "        models['en_skills'] = spacy.load('en_skills_model')\n",
        "    except:\n",
        "        models['en_skills'] = None\n",
        "\n",
        "    try:\n",
        "        models['fr_skills'] = spacy.load('fr_skills_model')\n",
        "    except:\n",
        "        models['fr_skills'] = None\n",
        "\n",
        "    return models\n",
        "\n",
        "# Extraction d'entités avec modèle combiné\n",
        "def extract_entities_combined(text, models, lang_code):\n",
        "    base_model = models['fr'] if lang_code == 'fr' else models['en']\n",
        "    base_doc = base_model(text)\n",
        "\n",
        "    # Si le modèle personnalisé est disponible, l'utiliser aussi\n",
        "    custom_entities = []\n",
        "    if models['custom']:\n",
        "        custom_doc = models['custom'](text)\n",
        "        custom_entities = [(ent.text, ent.label_) for ent in custom_doc.ents]\n",
        "\n",
        "    # Combiner les entités\n",
        "    combined_entities = [(ent.text, ent.label_) for ent in base_doc.ents]\n",
        "    combined_entities.extend(custom_entities)\n",
        "\n",
        "    return base_doc, combined_entities\n",
        "\n",
        "# Extraction du nom\n",
        "def extract_name(doc, entities):\n",
        "    for text, label in entities:\n",
        "        if label in ('PERSON', 'PER'):\n",
        "            tokens = text.split()\n",
        "            if len(tokens) >= 2:\n",
        "                return tokens[0], ' '.join(tokens[1:])\n",
        "    return \"\", \"\"\n",
        "\n",
        "# Extraction de l'email\n",
        "def extract_email(doc):\n",
        "    matcher = spacy.matcher.Matcher(doc.vocab)\n",
        "    matcher.add('EMAIL', [[{'LIKE_EMAIL': True}]])\n",
        "    for _, start, end in matcher(doc):\n",
        "        return doc[start:end].text\n",
        "    return \"\"\n",
        "\n",
        "# Extraction du numéro de téléphone\n",
        "def extract_phone(text, lang_code):\n",
        "    if lang_code == 'fr':\n",
        "        patterns = [\n",
        "            r\"\\b(?:\\+33\\s*(?:\\(0\\)\\s*)?|0)[1-9](?:[\\s.\\-/]*\\d{2}){4}\\b\"\n",
        "        ]\n",
        "    else:\n",
        "        patterns = [\n",
        "            r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\d{3}[-.\\s]?){2}\\d{4}\\b\"\n",
        "        ]\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, text)\n",
        "        if m:\n",
        "            return m.group()\n",
        "\n",
        "    # Pattern générique au cas où\n",
        "    fb = re.search(r\"\\b[+\\d][\\d\\s().-]{6,}\\d\\b\", text)\n",
        "    return fb.group() if fb else \"\"\n",
        "\n",
        "# Extraction de sections génériques\n",
        "def extract_section(text, header, lang_code='fr'):\n",
        "    # Tenir compte des variations d'accentuation et de casse\n",
        "    header_pattern = f\"{header}\"\n",
        "    pattern = rf\"{header_pattern}\\s*[:\\n](.*?)(?=\\n[A-ZÉÈÀÂÔÛÊÇ][a-zéèàâêîôûç]*[:\\n]|$)\"\n",
        "    m = re.search(pattern, text, re.S | re.IGNORECASE)\n",
        "\n",
        "    if not m:\n",
        "        # Essayer avec un pattern plus souple\n",
        "        pattern = rf\"{header_pattern}.*?\\n(.*?)(?=\\n\\s*[A-ZÉÈÀÂÔÛÊÇ][a-zéèàâêîôûç]*|$)\"\n",
        "        m = re.search(pattern, text, re.S | re.IGNORECASE)\n",
        "\n",
        "    return m.group(1).strip() if m else \"\"\n",
        "\n",
        "# Extraction du profil\n",
        "def extract_profile(text, lang_code):\n",
        "    headers = ['Profil', 'À propos', 'About', 'Summary', 'Résumé', 'Présentation']\n",
        "    for header in headers:\n",
        "        section = extract_section(text, header, lang_code)\n",
        "        if section:\n",
        "            return section\n",
        "\n",
        "    # Si aucune section trouvée, essayer d'extraire les premiers paragraphes\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    if len(paragraphs) > 1:\n",
        "        for p in paragraphs[:3]:  # Regarder les 3 premiers paragraphes\n",
        "            if len(p.split()) > 15:  # Au moins 15 mots\n",
        "                return p\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Extraction des compétences\n",
        "def extract_skills(doc, text, entities, models, lang_code):\n",
        "    # Extraire la section compétences\n",
        "    headers = ['Compétences', 'Skills', 'Expertise', 'Technical Skills']\n",
        "    section = \"\"\n",
        "    for header in headers:\n",
        "        section = extract_section(text, header, lang_code)\n",
        "        if section:\n",
        "            break\n",
        "\n",
        "    # Extraire les éléments de la section\n",
        "    items = re.split(r\"[\\n,•]+\", section)\n",
        "    skills = [i.strip() for i in items if i.strip()]\n",
        "\n",
        "    # Utiliser le modèle de compétences si disponible\n",
        "    skills_model_key = 'fr_skills' if lang_code == 'fr' else 'en_skills'\n",
        "    if models.get(skills_model_key):\n",
        "        skills_doc = models[skills_model_key](text)\n",
        "        for ent in skills_doc.ents:\n",
        "            if ent.label_ == 'SKILL':\n",
        "                skills.append(ent.text)\n",
        "\n",
        "    # Utiliser le modèle personnalisé pour les compétences\n",
        "    if models['custom']:\n",
        "        custom_doc = models['custom'](text)\n",
        "        for ent in custom_doc.ents:\n",
        "            if ent.label_ in ['SKILL', 'COMPETENCE', 'TECHNOLOGY']:\n",
        "                skills.append(ent.text)\n",
        "\n",
        "    # Retirer les doublons et les éléments vides\n",
        "    unique_skills = []\n",
        "    for skill in skills:\n",
        "        cleaned = skill.strip()\n",
        "        if cleaned and cleaned not in unique_skills:\n",
        "            unique_skills.append(cleaned)\n",
        "\n",
        "    return unique_skills\n",
        "\n",
        "# Extraction des formations\n",
        "def extract_education(text, lang_code):\n",
        "    headers = ['Formation', 'Education', 'Études', 'Parcours académique']\n",
        "    section = \"\"\n",
        "    for header in headers:\n",
        "        section = extract_section(text, header, lang_code)\n",
        "        if section:\n",
        "            break\n",
        "\n",
        "    lines = [l.strip() for l in section.splitlines() if l.strip()]\n",
        "\n",
        "    educations = []\n",
        "    current_education = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        # Détecter si c'est une nouvelle entrée (date ou diplôme)\n",
        "        if re.search(r'\\b\\d{4}\\b', line) or any(edu_term in line.lower() for edu_term in ['diplôme', 'master', 'licence', 'bac', 'certificat', 'brevet', 'ingénieur', 'bachelor', 'degree']):\n",
        "            if current_education:\n",
        "                educations.append(current_education)\n",
        "            current_education = line\n",
        "        else:\n",
        "            if current_education:\n",
        "                current_education += \" | \" + line\n",
        "            else:\n",
        "                current_education = line\n",
        "\n",
        "    if current_education:\n",
        "        educations.append(current_education)\n",
        "\n",
        "    return educations\n",
        "\n",
        "# Extraction des certifications\n",
        "def extract_certifications(text, lang_code):\n",
        "    headers = ['Certifications', 'Certificats', 'Accréditations']\n",
        "    section = \"\"\n",
        "    for header in headers:\n",
        "        section = extract_section(text, header, lang_code)\n",
        "        if section:\n",
        "            break\n",
        "\n",
        "    items = re.split(r\"[\\n•]+\", section)\n",
        "    return [item.strip() for item in items if item.strip()]\n",
        "\n",
        "# Extraction des langues\n",
        "def extract_languages(text, lang_code):\n",
        "    headers = ['Langues', 'Languages', 'Compétences linguistiques']\n",
        "    section = \"\"\n",
        "    for header in headers:\n",
        "        section = extract_section(text, header, lang_code)\n",
        "        if section:\n",
        "            break\n",
        "\n",
        "    items = re.split(r\"[\\n,•]+\", section)\n",
        "    languages = []\n",
        "    for item in items:\n",
        "        lang = item.strip(' •\\u2022').strip()\n",
        "        if lang:\n",
        "            languages.append(lang)\n",
        "\n",
        "    return languages\n",
        "\n",
        "# Détection du niveau d'expérience\n",
        "def extract_experience_level(doc, text, entities):\n",
        "    # Chercher d'abord des mentions directes d'années d'expérience\n",
        "    years_pattern = r\"(\\d+)[\\s+](?:ans|années|an|year|years)[\\s+](?:d'expérience|d'expériences|experience)\"\n",
        "    years_match = re.search(years_pattern, text, re.IGNORECASE)\n",
        "\n",
        "    if years_match:\n",
        "        years = int(years_match.group(1))\n",
        "        if years < 3:\n",
        "            return \"Junior\"\n",
        "        elif years < 8:\n",
        "            return \"Intermédiaire\"\n",
        "        else:\n",
        "            return \"Senior\"\n",
        "\n",
        "    # Rechercher des verbes liés au leadership\n",
        "    leadership_verbs = ['diriger', 'manager', 'superviser', 'lead', 'manage', 'direct', 'oversee']\n",
        "    mid_verbs = ['développer', 'concevoir', 'analyser', 'implémenter', 'develop', 'design', 'analyze', 'implement']\n",
        "\n",
        "    verbs = [t.text.lower() for t in doc if t.pos_ == 'VERB']\n",
        "\n",
        "    if any(lv in ' '.join(verbs).lower() for lv in leadership_verbs):\n",
        "        return \"Senior\"\n",
        "    elif any(mv in ' '.join(verbs).lower() for mv in mid_verbs):\n",
        "        return \"Intermédiaire\"\n",
        "\n",
        "    return \"Junior\"\n",
        "\n",
        "# Calcul du score de correspondance\n",
        "def compute_matching_score(text, job_keywords=None):\n",
        "    # Liste par défaut de mots-clés\n",
        "    default_keywords = ['python', 'sql', 'data', 'machine learning', 'analyse', 'project']\n",
        "\n",
        "    # Utiliser les mots-clés fournis ou les mots-clés par défaut\n",
        "    keywords = job_keywords if job_keywords else default_keywords\n",
        "\n",
        "    # Créer un vecteur pour le texte et les mots-clés\n",
        "    vectorizer = CountVectorizer()\n",
        "    try:\n",
        "        vectors = vectorizer.fit_transform([text.lower(), ' '.join(keywords).lower()])\n",
        "        similarity = cosine_similarity(vectors)\n",
        "        return round(similarity[0][1] * 100, 2)\n",
        "    except:\n",
        "        # En cas d'erreur, retourner un score par défaut\n",
        "        return 50.0\n",
        "\n",
        "# Traitement des fichiers PDF\n",
        "def process_pdf_files(uploaded_files, models, lang, job_keywords=None):\n",
        "    lang_code = 'fr' if lang == 'French' else 'en'\n",
        "    results = []\n",
        "\n",
        "    with st.spinner('Traitement des CVs en cours...'):\n",
        "        progress_bar = st.progress(0)\n",
        "\n",
        "        for i, uploaded in enumerate(uploaded_files):\n",
        "            try:\n",
        "                # Lecture du PDF\n",
        "                pdf = fitz.open(stream=uploaded.read(), filetype='pdf')\n",
        "                text = '\\n'.join(page.get_text() for page in pdf)\n",
        "\n",
        "                # Extraction des entités combinées\n",
        "                doc, entities = extract_entities_combined(text, models, lang_code)\n",
        "\n",
        "                # Extraction des informations\n",
        "                first_name, last_name = extract_name(doc, entities)\n",
        "                profile = extract_profile(text, lang_code)\n",
        "                skills = extract_skills(doc, text, entities, models, lang_code)\n",
        "                education = extract_education(text, lang_code)\n",
        "                certifications = extract_certifications(text, lang_code)\n",
        "                languages = extract_languages(text, lang_code)\n",
        "                experience_level = extract_experience_level(doc, text, entities)\n",
        "\n",
        "                # Calcul du score de correspondance\n",
        "                score = compute_matching_score(text, job_keywords)\n",
        "\n",
        "                # Ajout des résultats\n",
        "                results.append({\n",
        "                    'Nom du fichier': uploaded.name,\n",
        "                    'Prénom' if lang_code == 'fr' else 'First Name': first_name,\n",
        "                    'Nom' if lang_code == 'fr' else 'Last Name': last_name,\n",
        "                    'Profil' if lang_code == 'fr' else 'Profile': profile[:200] + '...' if len(profile) > 200 else profile,\n",
        "                    'Email': extract_email(doc),\n",
        "                    'Téléphone' if lang_code == 'fr' else 'Phone': extract_phone(text, lang_code),\n",
        "                    'Compétences' if lang_code == 'fr' else 'Skills': skills,\n",
        "                    'Formations' if lang_code == 'fr' else 'Education': education,\n",
        "                    'Certifications': certifications,\n",
        "                    'Langues' if lang_code == 'fr' else 'Languages': languages,\n",
        "                    'Niveau expérience' if lang_code == 'fr' else 'Experience Level': experience_level,\n",
        "                    'Score': score\n",
        "                })\n",
        "\n",
        "                # Mise à jour de la barre de progression\n",
        "                progress_bar.progress((i + 1) / len(uploaded_files))\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Erreur lors du traitement de {uploaded.name}: {str(e)}\")\n",
        "\n",
        "    progress_bar.empty()\n",
        "    return results\n",
        "\n",
        "# Interface utilisateur principale\n",
        "def main():\n",
        "    # Chargement des modèles\n",
        "    models = load_spacy_models()\n",
        "\n",
        "    # En-tête\n",
        "    st.markdown(\"<div class='main-header'>📊 RH Analytics - Analyse intelligente de CV</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    # Menu latéral\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"## ⚙️ Paramètres\")\n",
        "\n",
        "        # Sélection de la langue\n",
        "        lang = st.selectbox('Langue des CVs', ['French', 'English'], index=0)\n",
        "\n",
        "        # Section des mots-clés de poste\n",
        "        st.markdown(\"### 🎯 Mots-clés du poste\")\n",
        "        default_keywords = \"python, sql, data, machine learning, analyse, project\"\n",
        "        job_keywords = st.text_area(\n",
        "            \"Entrez les mots-clés du poste (séparés par des virgules)\",\n",
        "            value=default_keywords,\n",
        "            help=\"Ces mots-clés seront utilisés pour calculer un score de correspondance\"\n",
        "        )\n",
        "        job_keywords_list = [k.strip() for k in job_keywords.split(',') if k.strip()]\n",
        "\n",
        "        # Section de téléchargement\n",
        "        st.markdown(\"### 📤 Téléchargement des CVs\")\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"Téléversez les CVs (format PDF)\",\n",
        "            type=['pdf'],\n",
        "            accept_multiple_files=True,\n",
        "            help=\"Vous pouvez télécharger plusieurs fichiers PDF\"\n",
        "        )\n",
        "\n",
        "        if uploaded_files:\n",
        "            st.success(f\"✅ {len(uploaded_files)} fichier(s) téléversé(s)\")\n",
        "\n",
        "    # Corps principal\n",
        "    if uploaded_files:\n",
        "        # Traitement des fichiers\n",
        "        results = process_pdf_files(uploaded_files, models, lang, job_keywords_list)\n",
        "\n",
        "        if results:\n",
        "            # Convertir les résultats en DataFrame\n",
        "            df = pd.DataFrame(results)\n",
        "\n",
        "            # Création de tabs pour l'affichage des résultats\n",
        "            tab1, tab2, tab3 = st.tabs([\"📋 Vue d'ensemble\", \"📊 Statistiques\", \"👤 Profils détaillés\"])\n",
        "\n",
        "            with tab1:\n",
        "                st.markdown(\"<div class='sub-header'>📋 Aperçu des candidats</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Colonnes importantes pour la vue d'ensemble - selon les spécifications demandées\n",
        "                overview_cols = [\n",
        "                    'Nom' if lang == 'French' else 'Last Name',\n",
        "                    'Prénom' if lang == 'French' else 'First Name',\n",
        "                    'Email',\n",
        "                    'Téléphone' if lang == 'French' else 'Phone',\n",
        "                    'Profil' if lang == 'French' else 'Profile',\n",
        "                    'Formations' if lang == 'French' else 'Education',\n",
        "                    'Compétences' if lang == 'French' else 'Skills',\n",
        "                    'Langues' if lang == 'French' else 'Languages',\n",
        "                    'Niveau expérience' if lang == 'French' else 'Experience Level'\n",
        "                ]\n",
        "\n",
        "                # Affichage du DataFrame filtré\n",
        "                st.dataframe(df[overview_cols], use_container_width=True)\n",
        "\n",
        "                # Section de téléchargement des résultats\n",
        "                st.markdown(\"### 📥 Télécharger les résultats\")\n",
        "                sep = ';' if lang == 'French' else ','\n",
        "                csv_data = df.to_csv(index=False, sep=sep)\n",
        "\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.download_button(\n",
        "                        label=\"📥 Télécharger CSV\",\n",
        "                        data=csv_data,\n",
        "                        file_name=f'resultats_cv_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv',\n",
        "                        mime='text/csv'\n",
        "                    )\n",
        "                with col2:\n",
        "                    # Format Excel pour préserver les listes\n",
        "                    excel_filename = f'resultats_cv_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
        "\n",
        "                    # Création d'un buffer binaire en mémoire pour Excel\n",
        "                    buffer = BytesIO()\n",
        "\n",
        "                    # Sauvegarde du dataframe en Excel\n",
        "                    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
        "                        df.to_excel(writer, index=False, sheet_name='Résultats')\n",
        "\n",
        "                    # Préparation des données pour le téléchargement\n",
        "                    buffer.seek(0)\n",
        "\n",
        "                    st.download_button(\n",
        "                        label=\"📥 Télécharger Excel\",\n",
        "                        data=buffer,\n",
        "                        file_name=excel_filename,\n",
        "                        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
        "                    )\n",
        "\n",
        "            with tab2:\n",
        "                st.markdown(\"<div class='sub-header'>📊 Statistiques et insights</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Métriques clés\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                with col1:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.markdown(f\"<div class='metric-value'>{len(df)}</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"<div class='metric-label'>Candidats analysés</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                with col2:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.markdown(f\"<div class='metric-value'>{df['Score'].mean():.1f}%</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"<div class='metric-label'>Score moyen de correspondance</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                with col3:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    top_score = df['Score'].max()\n",
        "                    top_candidates = df[df['Score'] == top_score].shape[0]\n",
        "                    st.markdown(f\"<div class='metric-value'>{top_score:.1f}%</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(f\"<div class='metric-label'>Score le plus élevé ({top_candidates} candidat{'s' if top_candidates > 1 else ''})</div>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Graphiques\n",
        "                col1, col2 = st.columns(2)\n",
        "\n",
        "                with col1:\n",
        "                    # Distribution des scores\n",
        "                    fig_scores = px.histogram(\n",
        "                        df,\n",
        "                        x='Score',\n",
        "                        nbins=10,\n",
        "                        title=\"Distribution des scores\",\n",
        "                        labels={'Score': 'Score de correspondance (%)'},\n",
        "                        color_discrete_sequence=['#1E88E5']\n",
        "                    )\n",
        "                    fig_scores.update_layout(bargap=0.1)\n",
        "                    st.plotly_chart(fig_scores, use_container_width=True)\n",
        "\n",
        "                with col2:\n",
        "                    # Répartition des niveaux d'expérience\n",
        "                    exp_col = 'Niveau expérience' if lang == 'French' else 'Experience Level'\n",
        "                    exp_counts = df[exp_col].value_counts().reset_index()\n",
        "                    exp_counts.columns = ['Niveau', 'Nombre']\n",
        "\n",
        "                    fig_exp = px.pie(\n",
        "                        exp_counts,\n",
        "                        values='Nombre',\n",
        "                        names='Niveau',\n",
        "                        title=\"Répartition des niveaux d'expérience\",\n",
        "                        color_discrete_sequence=px.colors.qualitative.Set2\n",
        "                    )\n",
        "                    fig_exp.update_traces(textposition='inside', textinfo='percent+label')\n",
        "                    st.plotly_chart(fig_exp, use_container_width=True)\n",
        "\n",
        "                # Analyse des compétences\n",
        "                st.markdown(\"#### 💡 Analyse des compétences\")\n",
        "                skills_col = 'Compétences' if lang == 'French' else 'Skills'\n",
        "\n",
        "                # Extraction de toutes les compétences\n",
        "                all_skills = []\n",
        "                for skills_list in df[skills_col]:\n",
        "                    all_skills.extend(skills_list)\n",
        "\n",
        "                # Comptage des compétences\n",
        "                skill_counts = pd.Series(all_skills).value_counts().head(15)\n",
        "\n",
        "                # Graphique des compétences les plus fréquentes\n",
        "                fig_skills = px.bar(\n",
        "                    x=skill_counts.values,\n",
        "                    y=skill_counts.index,\n",
        "                    orientation='h',\n",
        "                    title=\"Top 15 des compétences les plus mentionnées\",\n",
        "                    labels={'x': 'Nombre de mentions', 'y': 'Compétence'},\n",
        "                    color=skill_counts.values,\n",
        "                    color_continuous_scale=px.colors.sequential.Blues\n",
        "                )\n",
        "                st.plotly_chart(fig_skills, use_container_width=True)\n",
        "\n",
        "                # Suggestions basées sur l'analyse\n",
        "                st.markdown(\"<div class='success-box'>\", unsafe_allow_html=True)\n",
        "                st.markdown(\"#### 🔍 Insights pour le recrutement\")\n",
        "\n",
        "                top_skills = skill_counts.head(5).index.tolist()\n",
        "                top_skills_str = \", \".join(top_skills)\n",
        "\n",
        "                st.markdown(f\"\"\"\n",
        "                * Les compétences les plus répandues parmi les candidats sont: **{top_skills_str}**\n",
        "                * {df[exp_col].value_counts().idxmax()} est le niveau d'expérience le plus représenté ({df[exp_col].value_counts().max()} candidats)\n",
        "                * {len(df[df['Score'] > 75])} candidats ont un score de correspondance supérieur à 75%\n",
        "                \"\"\")\n",
        "                st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "            with tab3:\n",
        "                st.markdown(\"<div class='sub-header'>👤 Profils détaillés des candidats</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Sélection du candidat\n",
        "                candidate_idx = st.selectbox(\n",
        "                    \"Sélectionnez un candidat\",\n",
        "                    options=range(len(df)),\n",
        "                    format_func=lambda i: f\"{df.iloc[i]['Prénom' if lang == 'French' else 'First Name']} {df.iloc[i]['Nom' if lang == 'French' else 'Last Name']} - {df.iloc[i]['Nom du fichier']}\"\n",
        "                )\n",
        "\n",
        "                # Affichage du profil détaillé\n",
        "                candidate = df.iloc[candidate_idx]\n",
        "\n",
        "                col1, col2 = st.columns([2, 1])\n",
        "\n",
        "                with col1:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.subheader(f\"{candidate['Prénom' if lang == 'French' else 'First Name']} {candidate['Nom' if lang == 'French' else 'Last Name']}\")\n",
        "                    st.caption(f\"📄 {candidate['Nom du fichier']}\")\n",
        "\n",
        "                    # Informations de contact\n",
        "                    st.markdown(\"#### 📞 Contact\")\n",
        "                    st.markdown(f\"**Email:** {candidate['Email']}\")\n",
        "                    st.markdown(f\"**Téléphone:** {candidate['Téléphone' if lang == 'French' else 'Phone']}\")\n",
        "\n",
        "                    # Profil\n",
        "                    st.markdown(\"#### 👤 Profil\")\n",
        "                    st.markdown(candidate['Profil' if lang == 'French' else 'Profile'])\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                with col2:\n",
        "                    # Score et niveau d'expérience\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"#### 📊 Évaluation\")\n",
        "                    score_color = \"green\" if candidate['Score'] >= 75 else \"orange\" if candidate['Score'] >= 50 else \"red\"\n",
        "                    st.markdown(f\"**Score de correspondance:** <span style='color:{score_color};font-weight:bold;'>{candidate['Score']}%</span>\", unsafe_allow_html=True)\n",
        "                    st.markdown(f\"**Niveau d'expérience:** {candidate['Niveau expérience' if lang == 'French' else 'Experience Level']}\")\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Compétences\n",
        "                st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                st.markdown(\"#### 🛠️ Compétences\")\n",
        "\n",
        "                # Affichage des compétences avec mise en évidence des mots-clés\n",
        "                skills_list = candidate['Compétences' if lang == 'French' else 'Skills']\n",
        "                if job_keywords_list and skills_list:\n",
        "                    for skill in skills_list:\n",
        "                        if any(kw.lower() in skill.lower() for kw in job_keywords_list):\n",
        "                            st.markdown(f\"- <span style='background-color:#E3F2FD;padding:2px 5px;border-radius:3px;'>{skill}</span>\", unsafe_allow_html=True)\n",
        "                        else:\n",
        "                            st.markdown(f\"- {skill}\")\n",
        "                else:\n",
        "                    for skill in skills_list:\n",
        "                        st.markdown(f\"- {skill}\")\n",
        "                st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Formation et certifications\n",
        "                col1, col2 = st.columns(2)\n",
        "\n",
        "                with col1:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"#### 🎓 Formation\")\n",
        "                    education_list = candidate['Formations' if lang == 'French' else 'Education']\n",
        "                    if education_list:\n",
        "                        for edu in education_list:\n",
        "                            st.markdown(f\"- {edu}\")\n",
        "                    else:\n",
        "                        st.markdown(\"*Aucune formation spécifiée*\")\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                with col2:\n",
        "                    st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                    st.markdown(\"#### 📜 Certifications\")\n",
        "                    cert_list = candidate['Certifications']\n",
        "                    if cert_list:\n",
        "                        for cert in cert_list:\n",
        "                            st.markdown(f\"- {cert}\")\n",
        "                    else:\n",
        "                        st.markdown(\"*Aucune certification spécifiée*\")\n",
        "                    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Langues\n",
        "                st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                st.markdown(\"#### 🌍 Langues\")\n",
        "                lang_list = candidate['Langues' if lang == 'French' else 'Languages']\n",
        "                if lang_list:\n",
        "                    col1, col2, col3 = st.columns([1, 1, 1])\n",
        "                    for i, lang_item in enumerate(lang_list):\n",
        "                        with col1 if i % 3 == 0 else col2 if i % 3 == 1 else col3:\n",
        "                            st.markdown(f\"- {lang_item}\")\n",
        "                else:\n",
        "                    st.markdown(\"*Aucune langue spécifiée*\")\n",
        "                st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # Recommandation RH\n",
        "                st.markdown(\"<div class='card'>\", unsafe_allow_html=True)\n",
        "                st.markdown(\"#### 💼 Recommandation RH\")\n",
        "\n",
        "                # Calcul d'une recommandation basée sur le score et l'expérience\n",
        "                if candidate['Score'] >= 80:\n",
        "                    recommendation = \"⭐⭐⭐ **Excellent candidat** - À contacter en priorité\"\n",
        "                elif candidate['Score'] >= 65:\n",
        "                    recommendation = \"⭐⭐ **Bon candidat** - À considérer sérieusement\"\n",
        "                elif candidate['Score'] >= 50:\n",
        "                    recommendation = \"⭐ **Candidat potentiel** - À considérer si les meilleurs profils ne sont pas disponibles\"\n",
        "                else:\n",
        "                    recommendation = \"❌ **Profil non adapté** - Ne correspond pas aux critères du poste\"\n",
        "\n",
        "                st.markdown(recommendation)\n",
        "\n",
        "                # Actions RH\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                with col1:\n",
        "                    st.button(\"📞 Planifier un entretien\", key=f\"interview_{candidate_idx}\")\n",
        "                with col2:\n",
        "                    st.button(\"✉️ Contacter par email\", key=f\"email_{candidate_idx}\")\n",
        "                with col3:\n",
        "                    st.button(\"❌ Rejeter la candidature\", key=f\"reject_{candidate_idx}\")\n",
        "\n",
        "                st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "def customize_interface():\n",
        "    \"\"\"Applique des personnalisations supplémentaires à l'interface\"\"\"\n",
        "    # Ajout d'un logo ou d'une image d'en-tête (si nécessaire)\n",
        "    # Pourrait être complété par une image de l'entreprise ou un logo RH\n",
        "    pass\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "2Z9xF-PPwC9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}